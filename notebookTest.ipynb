{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden_size: 4, Learning-rate: 0.3, Best-Accuracy-Train: 1.0000, Best-Accuracy-Test: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import NetMonk\n",
    "import LoadDataMonk\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display as display\n",
    "\n",
    "print(\"TRAINING MONK DATASET\")\n",
    "# NET TYPE\n",
    "netTypeList = {0: \"Classifier\", 1: \"Regressor\"}\n",
    "netType = 1 # 0 for Classifier, 1 for Regressor\n",
    "# HYPERPARAMETER\n",
    "interval = 0.25\n",
    "learning_rate = 0.3\n",
    "hidden_units = 4\n",
    "num_epochs = 400\n",
    "momentum = 0.8\n",
    "# PATH\n",
    "pathTrain = \"MONK/monks-2.train\"\n",
    "pathTest = \"MONK/monks-2.test\"\n",
    "pathName = f'modelsMonk/Monk2-{hidden_units}-{learning_rate}-{netTypeList[netType]}'\n",
    "# IMPORT DATA\n",
    "dataMonk = LoadDataMonk.DataMonk(pathTrain, pathTest)\n",
    "# DATA: TENSOR, GPU, DATALOADER\n",
    "#dataMonk.convertY()\n",
    "dataMonk.convertToTensor()\n",
    "dataMonk.moveToGpu()\n",
    "data_loader_train, data_loader_test = dataMonk.createDataLoader()\n",
    "# CREATE NET\n",
    "# Classifier or Regressor\n",
    "if netTypeList[netType] == \"Classifier\":\n",
    "    print(\"Load classifier [net]\")\n",
    "    net = NetMonk.NetMonkClassifier(hidden_units)\n",
    "else:\n",
    "    print(\"Load regressor [net]\")\n",
    "    net = NetMonk.NetMonkRegressor(interval, hidden_units)\n",
    "# MOVE NET TO GPU\n",
    "net = net.to(\"cuda:0\")\n",
    "# SET TYPE NET\n",
    "net = net.double()\n",
    "# OPTIMIZER AND CRITERION\n",
    "# MSELoss for Regressor, CrossEntropyLoss for Classifier\n",
    "# SGD for Regressor, Adam for Classifier\n",
    "if netTypeList[netType] == \"Classifier\":\n",
    "    print(\"Load CrossEntropyLoss [criterion]\\nLoad Adam [optimizer]\")\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "else:\n",
    "    print(\"Load MSELoss [criterion]\\nLoad SGD [optimizer]\")\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=momentum)\n",
    "\n",
    "# CREATE DIR\n",
    "os.makedirs(pathName, exist_ok=True)    \n",
    "\n",
    "# MODEL SAVE\n",
    "torch.save(net, f'{pathName}/model.pth')\n",
    "with open(f'{pathName}/model_parameters.txt', 'w') as file:\n",
    "    file.write('Pesi layer1\\n')\n",
    "    file.write(str(net.layer1.weight.data) + '\\n')\n",
    "    file.write('Bias layer1\\n')\n",
    "    file.write(str(net.layer1.bias.data) + '\\n')\n",
    "    file.write('Pesi layer2\\n')\n",
    "    file.write(str(net.layer2.weight.data) + '\\n')\n",
    "    file.write('Bias layer2\\n')\n",
    "    file.write(str(net.layer2.bias.data) + '\\n')\n",
    "\n",
    "#Values used for graphs\n",
    "loss_values_train = []\n",
    "accuracy_values_train = []\n",
    "loss_values_test = []\n",
    "accuracy_values_test = []\n",
    "# BEST\n",
    "best_accuracy_train = 0.0\n",
    "best_accuracy_test = 0.0\n",
    "\n",
    "net.train()\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    total = 0\n",
    "    correct = 0\n",
    "\n",
    "    for batch_input, batch_output in data_loader_train:\n",
    "        #Forward pass\n",
    "        outputs = net(batch_input)\n",
    "        #Training loss\n",
    "        loss = None\n",
    "        if netTypeList[netType] == \"Classifier\":\n",
    "            batch_output_squeeze = batch_output.squeeze().long()\n",
    "            loss = criterion(outputs, batch_output_squeeze)\n",
    "        else:\n",
    "            loss = criterion(outputs, batch_output)\n",
    "        #Calculate total loss\n",
    "        total_loss += loss.item()\n",
    "        #Backward and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #\n",
    "        if netTypeList[netType] == \"Classifier\":\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == batch_output_squeeze).sum().item()\n",
    "        else:\n",
    "            #predicted = torch.sign(outputs)\n",
    "            predicted = (outputs > 0.5).float()\n",
    "            correct += (predicted == batch_output).sum().item()\n",
    "        total += batch_input.size(0)\n",
    "    accuracy = correct / total\n",
    "    best_accuracy_train = max(best_accuracy_train, accuracy)\n",
    "    accuracy_values_train.append(accuracy)\n",
    "    avg_loss = total_loss / len(data_loader_train)\n",
    "    #Add to list\n",
    "    loss_values_train.append(avg_loss)\n",
    "\n",
    "\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    #CALCULATE ACCURACY VAL\n",
    "    net.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_input, batch_output in data_loader_test:\n",
    "            outputs = net(batch_input)\n",
    "            loss = None\n",
    "            if netTypeList[netType] == \"Classifier\":\n",
    "                batch_output_squeeze = batch_output.squeeze().long()\n",
    "                loss = criterion(outputs, batch_output_squeeze)\n",
    "            else:\n",
    "                loss = criterion(outputs, batch_output)\n",
    "            total_loss += loss.item()\n",
    "            if netTypeList[netType] == \"Classifier\":\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                correct += (predicted == batch_output_squeeze).sum().item()\n",
    "            else:\n",
    "                #predicted = torch.sign(outputs)\n",
    "                predicted = (outputs > 0.5).float()\n",
    "                correct += (predicted == batch_output).sum().item()\n",
    "            total += batch_input.size(0)\n",
    "        accuracy = correct / total\n",
    "        best_accuracy_test = max(best_accuracy_test, accuracy)\n",
    "        accuracy_values_test.append(accuracy)\n",
    "        avg_loss = total_loss / len(data_loader_test)\n",
    "        loss_values_test.append(avg_loss)\n",
    "    net.train()\n",
    "\n",
    "#Save plot loss\n",
    "display.clear_output(wait=True)\n",
    "plt.plot(loss_values_train, label='Training Loss')\n",
    "plt.plot(loss_values_test, label = 'Test loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss per Epoch')\n",
    "plt.legend()\n",
    "plt.savefig(f'{pathName}/Loss.png')\n",
    "plt.clf()\n",
    "\n",
    "#Save plot accuracy\n",
    "display.clear_output(wait=True)\n",
    "plt.plot(accuracy_values_train, label='Accuracy Train')\n",
    "plt.plot(accuracy_values_test, label='Accuracy Test')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy for Epoch')\n",
    "plt.legend()\n",
    "plt.savefig(f'{pathName}/Accuracy-test.png')\n",
    "plt.clf()\n",
    "\n",
    "result = f'Hidden_size: {hidden_units}, Learning-rate: {learning_rate}, Best-Accuracy-Train: {best_accuracy_train:.4f}, Best-Accuracy-Test: {best_accuracy_test:.4f}'\n",
    "with open(f\"{pathName}/result.txt\", 'w') as file:\n",
    "    file.write(result + \"\\n\")\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
