{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import NetCup\n",
    "import LoadDataCup\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display as display\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "print(\"TRAINING CUP DATASET\")\n",
    "# HYPERPARAMETER\n",
    "#interval = 0.7\n",
    "learning_rate = 0.001\n",
    "num_epochs = 30000\n",
    "#momentum = 0.9\n",
    "threshold = 0.01\n",
    "#penality = 0.0005\n",
    "\n",
    "#grid search\n",
    "layers_conf = [[32, 64, 128, 256, 512], [32, 64, 128, 256, 256]]\n",
    "activation_functions = ['tanh']\n",
    "optimizers = ['sgd']\n",
    "penalities = [0.00005]\n",
    "momentums = [0.9]\n",
    "#\n",
    "bestResults = []\n",
    "\n",
    "for layers, activation, optimizerName, penality, momentum in product(layers_conf, activation_functions, optimizers, penalities, momentums):\n",
    "    layer1 = layers[0]\n",
    "    layer2 = layers[1]\n",
    "    layer3 = layers[2]\n",
    "    layer4 = layers[3]\n",
    "    layer5 = layers[4]\n",
    "    # PATH\n",
    "    pathTrain = \"CUP/ML-CUP23-TRAIN.csv\"\n",
    "    pathTestInput = \"CUP/ML-CUP23-TEST-INPUT.csv\"\n",
    "    pathTestTarget = \"CUP/ML-CUP23-TEST-TARGET.csv\"\n",
    "    testName = f\"[{layer1}-{layer2}-{layer3}-{layer4}-{layer5}]-{optimizerName}-{activation}-{penality}-{momentum}\"\n",
    "    pathName = f'modelsCup/Cup-{testName}'\n",
    "    # IMPORT DATA\n",
    "    dataCup = LoadDataCup.DataCup(pathTrain, pathTestInput, pathTestTarget)\n",
    "    # SPLIT SET\n",
    "    dataCup.splitData()\n",
    "    # DATA: TENSOR, GPU, DATALOADER\n",
    "    dataCup.convertToTensor()\n",
    "    # MOVE TO GPU\n",
    "    #device = \"mps\"\n",
    "    #dataCup.moveToGpu(device=device)\n",
    "    data_loader_train, data_loader_test = dataCup.createDataLoader()\n",
    "    # CREATE NET\n",
    "    # If you need to change the neurons number go to netCup.py\n",
    "    print(\"Load regressor [net]\")\n",
    "    net = NetCup.NetCupRegressor(layer1, layer2, layer3, layer4, layer5, activation)\n",
    "    #net = NetCup.NetCupCNN()\n",
    "    # MOVE NET TO GPU\n",
    "    #net = net.to(device)\n",
    "    # SET TYPE NET\n",
    "    net = net.float()\n",
    "    # OPTIMIZER AND CRITERION\n",
    "    # MSELoss for Regressor\n",
    "    # SGD for Regressor\n",
    "    print(\"Load MSELoss [criterion]\\nLoad SGD [optimizer]\")\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = None\n",
    "    if optimizerName == \"adam\":\n",
    "        optimizer = optim.Adam(net.parameters(), lr=learning_rate, weight_decay=penality)\n",
    "    elif optimizerName == \"sgd\":\n",
    "        optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=momentum, weight_decay=penality)\n",
    "    else:\n",
    "        print(\"OPTIMIZER NON TROVATO!\")\n",
    "        exit(1)\n",
    "\n",
    "    # CREATE DIR\n",
    "    os.makedirs(pathName, exist_ok=True)    \n",
    "\n",
    "    # MODEL SAVE\n",
    "    \"\"\"\n",
    "    with open(f'{pathName}/model_parameters.txt', 'w') as file:\n",
    "        file.write('Pesi layer1\\n')\n",
    "        file.write(str(net.layer1.weight.data) + '\\n')\n",
    "        file.write('Bias layer1\\n')\n",
    "        file.write(str(net.layer1.bias.data) + '\\n')\n",
    "        file.write('Pesi layer2\\n')\n",
    "        file.write(str(net.layer2.weight.data) + '\\n')\n",
    "        file.write('Bias layer2\\n')\n",
    "        file.write(str(net.layer2.bias.data) + '\\n')\n",
    "        file.write('Pesi layer3\\n')\n",
    "        file.write(str(net.layer3.weight.data) + '\\n')\n",
    "        file.write('Bias layer3\\n')\n",
    "        file.write(str(net.layer3.bias.data) + '\\n')\n",
    "        file.write('Pesi layer4\\n')\n",
    "        file.write(str(net.layer4.weight.data) + '\\n')\n",
    "        file.write('Bias layer4\\n')\n",
    "        file.write(str(net.layer4.bias.data) + '\\n')\n",
    "    \"\"\"\n",
    "    #Values used for graphs\n",
    "    loss_values_train = []\n",
    "    accuracy_values_train = []\n",
    "    loss_values_test = []\n",
    "    accuracy_values_test = []\n",
    "    # BEST\n",
    "    best_accuracy_train = 0.0\n",
    "    best_accuracy_test = 0.0\n",
    "    best_loss_train = 100.0\n",
    "    best_loss_test = 100.0\n",
    "    #\n",
    "    results = []\n",
    "    net.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        total = 0\n",
    "        correct = 0\n",
    "\n",
    "        for batch_input, batch_output in data_loader_train:\n",
    "            #Forward pass\n",
    "            outputs = net(batch_input)\n",
    "            #Training loss\n",
    "            loss = criterion(outputs, batch_output)\n",
    "            #Calculate total loss\n",
    "            total_loss += loss.item()\n",
    "            #Backward and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            #\n",
    "            difference = torch.abs(outputs - batch_output)\n",
    "            total += batch_input.size(0)\n",
    "            correct += torch.sum(difference < threshold)\n",
    "        accuracy_train = correct / total\n",
    "        avg_loss_train = total_loss / len(data_loader_train)\n",
    "        #Add to list\n",
    "        loss_values_train.append(avg_loss_train)\n",
    "\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        #CALCULATE ACCURACY VAL\n",
    "        net.eval()\n",
    "        total_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_input, batch_output in data_loader_test:\n",
    "                outputs = net(batch_input)\n",
    "                loss = criterion(outputs, batch_output)\n",
    "                total_loss += loss.item()\n",
    "                difference = torch.abs(outputs - batch_output)\n",
    "                total += batch_input.size(0)\n",
    "                correct += torch.sum(difference < threshold)\n",
    "            accuracy_test = correct / total\n",
    "            avg_loss_test = total_loss / len(data_loader_test)\n",
    "            loss_values_test.append(avg_loss_test)\n",
    "        net.train()\n",
    "        \n",
    "        result = f'Epoch[{epoch+1}/{num_epochs}] Learning-rate: {learning_rate}, Loss-Train: {avg_loss_train:.4f}, Loss-Test: {avg_loss_test:.4f}, Best-Accuracy-Train: {accuracy_train:.4f}, Best-Accuracy-Test: {accuracy_test:.4f}'\n",
    "        print(result)\n",
    "        \n",
    "        #Set best accuracy\n",
    "        best_accuracy_train = max(best_accuracy_train, accuracy_train)\n",
    "        best_accuracy_test = max(best_accuracy_test, accuracy_test)\n",
    "        #Set best loss\n",
    "        best_loss_train = min(best_loss_train, avg_loss_train)\n",
    "        best_loss_test = min(best_loss_test, avg_loss_test)\n",
    "        #List append\n",
    "        accuracy_values_train.append(accuracy_train.item())\n",
    "        accuracy_values_test.append(accuracy_test.item())\n",
    "        results.append(result)\n",
    "        \n",
    "\n",
    "    #Save model\n",
    "    torch.save(net, f'{pathName}/model.pth')\n",
    "    \n",
    "    #Save best results\n",
    "    bestPrint = f\"{testName}\\n   Best-loss-train: {best_loss_train:.4f}, Best-loss-test: {best_loss_test:.4f}, Best-Accuracy-Train: {best_accuracy_train:.4f}, Best-Accuracy-Test: {best_accuracy_test:.4f}\\n\"\n",
    "    bestResults.append(bestPrint)\n",
    "    \n",
    "    # Filter loss\n",
    "    #loss_values_train = utils.filterElement(loss_values_train)\n",
    "    #loss_values_test = utils.filterElement(loss_values_test)\n",
    "    \n",
    "    # Order in the same len\n",
    "    #min_len = min(len(loss_values_train), len(loss_values_test))\n",
    "    #loss_values_train = loss_values_train[:min_len]\n",
    "    #loss_values_test = loss_values_test[:min_len]\n",
    "\n",
    "    #Save plot loss\n",
    "    display.clear_output(wait=True)\n",
    "    plt.plot(loss_values_train, label='Training Loss')\n",
    "    plt.plot(loss_values_test, label = 'Test loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Loss per Epoch')\n",
    "    plt.ylim([0, 1.2])\n",
    "    plt.legend()\n",
    "    plt.savefig(f'{pathName}/Loss.png')\n",
    "    plt.clf()\n",
    "\n",
    "    #Save plot accuracy\n",
    "    display.clear_output(wait=True)\n",
    "    plt.plot(accuracy_values_train, label='Accuracy Train')\n",
    "    plt.plot(accuracy_values_test, label='Accuracy Test')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Accuracy for Epoch')\n",
    "    plt.legend()\n",
    "    plt.savefig(f'{pathName}/Accuracy-test.png')\n",
    "    plt.clf()\n",
    "\n",
    "\n",
    "    with open(f\"{pathName}/results.txt\", 'w') as file:\n",
    "        for res in results:\n",
    "            file.write(res + \"\\n\")\n",
    "    \n",
    "with open(\"Summary.txt\", \"w\") as file:\n",
    "    settings = f\"Grid Search Params: \\n {layers_conf} \\n {activation_functions} \\n {optimizers} \\n {penalities}\\n {momentums} \\n\\n\"\n",
    "    file.write(settings)\n",
    "    for best in bestResults:\n",
    "        file.write(best)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
