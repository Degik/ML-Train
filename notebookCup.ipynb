{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING CUP DATASET\n",
      "Batch size for training:  64\n",
      "Batch size for testing:  64\n",
      "Load regressor [net]\n",
      "Load MSELoss [criterion]\n",
      "Load SGD [optimizer]\n",
      "OPTIMIZER NON TROVATO!\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'zero_grad'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 118\u001b[0m\n\u001b[1;32m    116\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m    117\u001b[0m \u001b[38;5;66;03m#Backward and optimization\u001b[39;00m\n\u001b[0;32m--> 118\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_grad\u001b[49m()\n\u001b[1;32m    119\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m    120\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'zero_grad'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import NetCup\n",
    "import LoadDataCup\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display as display\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "print(\"TRAINING CUP DATASET\")\n",
    "# HYPERPARAMETER\n",
    "#interval = 0.7\n",
    "learning_rate = 0.001\n",
    "num_epochs = 3000\n",
    "momentum = 0.9\n",
    "threshold = 0.1\n",
    "#penality = 0.0005\n",
    "\n",
    "#grid search\n",
    "layers_conf = [[32, 64, 128], [16, 32, 64]]\n",
    "activation_functions = ['relu', 'tanh']\n",
    "optimizers = ['adam', 'sgd']\n",
    "penalities = [0.0005, 0.005]\n",
    "#\n",
    "\n",
    "\n",
    "for layers, activation, optimizerName, penality in product(layers_conf, activation_functions, optimizers, penalities):\n",
    "    layer1 = layers[0]\n",
    "    layer2 = layers[1]\n",
    "    layer3 = layers[2]\n",
    "    # PATH\n",
    "    pathTrain = \"CUP/ML-CUP23-TRAIN.csv\"\n",
    "    pathTestInput = \"CUP/ML-CUP23-TEST-INPUT.csv\"\n",
    "    pathTestTarget = \"CUP/ML-CUP23-TEST-TARGET.csv\"\n",
    "    pathName = f'modelsCup/Cup-[{layer1}-{layer2}-{layer3}]-{optimizerName}-{activation}-{penality}'\n",
    "    # IMPORT DATA\n",
    "    dataCup = LoadDataCup.DataCup(pathTrain, pathTestInput, pathTestTarget)\n",
    "    # SPLIT SET\n",
    "    dataCup.splitData()\n",
    "    # DATA: TENSOR, GPU, DATALOADER\n",
    "    dataCup.convertToTensor()\n",
    "    # MOVE TO GPU\n",
    "    device = \"mps\"\n",
    "    dataCup.moveToGpu(device=device)\n",
    "    data_loader_train, data_loader_test = dataCup.createDataLoader()\n",
    "    # CREATE NET\n",
    "    # If you need to change the neurons number go to netCup.py\n",
    "    print(\"Load regressor [net]\")\n",
    "    net = NetCup.NetCupRegressor(layer1, layer2, layer3, activation)\n",
    "    #net = NetCup.NetCupCNN()\n",
    "    # MOVE NET TO GPU\n",
    "    net = net.to(device)\n",
    "    # SET TYPE NET\n",
    "    net = net.float()\n",
    "    # OPTIMIZER AND CRITERION\n",
    "    # MSELoss for Regressor\n",
    "    # SGD for Regressor\n",
    "    print(\"Load MSELoss [criterion]\\nLoad SGD [optimizer]\")\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = None\n",
    "    if optimizerName == \"adam\":\n",
    "        optimizer = optim.Adam(net.parameters(), lr=learning_rate, weight_decay=penality)\n",
    "    elif optimizerName == \"sgd\":\n",
    "        optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=momentum, weight_decay=penality)\n",
    "    else:\n",
    "        print(\"OPTIMIZER NON TROVATO!\")\n",
    "        exit(1)\n",
    "\n",
    "    # CREATE DIR\n",
    "    os.makedirs(pathName, exist_ok=True)    \n",
    "\n",
    "    # MODEL SAVE\n",
    "    \"\"\"\n",
    "    with open(f'{pathName}/model_parameters.txt', 'w') as file:\n",
    "        file.write('Pesi layer1\\n')\n",
    "        file.write(str(net.layer1.weight.data) + '\\n')\n",
    "        file.write('Bias layer1\\n')\n",
    "        file.write(str(net.layer1.bias.data) + '\\n')\n",
    "        file.write('Pesi layer2\\n')\n",
    "        file.write(str(net.layer2.weight.data) + '\\n')\n",
    "        file.write('Bias layer2\\n')\n",
    "        file.write(str(net.layer2.bias.data) + '\\n')\n",
    "        file.write('Pesi layer3\\n')\n",
    "        file.write(str(net.layer3.weight.data) + '\\n')\n",
    "        file.write('Bias layer3\\n')\n",
    "        file.write(str(net.layer3.bias.data) + '\\n')\n",
    "        file.write('Pesi layer4\\n')\n",
    "        file.write(str(net.layer4.weight.data) + '\\n')\n",
    "        file.write('Bias layer4\\n')\n",
    "        file.write(str(net.layer4.bias.data) + '\\n')\n",
    "    \"\"\"\n",
    "    #Values used for graphs\n",
    "    loss_values_train = []\n",
    "    accuracy_values_train = []\n",
    "    loss_values_test = []\n",
    "    accuracy_values_test = []\n",
    "    # BEST\n",
    "    best_accuracy_train = 0.0\n",
    "    best_accuracy_test = 0.0\n",
    "    #\n",
    "    results = []\n",
    "    net.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        total = 0\n",
    "        correct = 0\n",
    "\n",
    "        for batch_input, batch_output in data_loader_train:\n",
    "            #Forward pass\n",
    "            outputs = net(batch_input)\n",
    "            #Training loss\n",
    "            loss = criterion(outputs, batch_output)\n",
    "            #Calculate total loss\n",
    "            total_loss += loss.item()\n",
    "            #Backward and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            #\n",
    "            difference = torch.abs(outputs - batch_output)\n",
    "            total += batch_input.size(0)\n",
    "            correct += torch.sum(difference < threshold)\n",
    "        accuracy_train = correct / total\n",
    "        avg_loss_train = total_loss / len(data_loader_train)\n",
    "        #Add to list\n",
    "        loss_values_train.append(avg_loss_train)\n",
    "\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        #CALCULATE ACCURACY VAL\n",
    "        net.eval()\n",
    "        total_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_input, batch_output in data_loader_test:\n",
    "                outputs = net(batch_input)\n",
    "                loss = criterion(outputs, batch_output)\n",
    "                total_loss += loss.item()\n",
    "                difference = torch.abs(outputs - batch_output)\n",
    "                total += batch_input.size(0)\n",
    "                correct += torch.sum(difference < threshold)\n",
    "            accuracy_test = correct / total\n",
    "            avg_loss_test = total_loss / len(data_loader_test)\n",
    "            loss_values_test.append(avg_loss_test)\n",
    "        net.train()\n",
    "        \n",
    "        result = f'Epoch[{epoch+1}/{num_epochs}] Learning-rate: {learning_rate}, Loss-Train: {avg_loss_train:.4f}, Loss-Test: {avg_loss_test:.4f}, Best-Accuracy-Train: {accuracy_train:.4f}, Best-Accuracy-Test: {accuracy_test:.4f}'\n",
    "        print(result)\n",
    "        \n",
    "        #Set best accuracy\n",
    "        best_accuracy_train = max(best_accuracy_train, accuracy_train)\n",
    "        best_accuracy_test = max(best_accuracy_test, accuracy_test)\n",
    "        \n",
    "        #List append\n",
    "        accuracy_values_train.append(accuracy_train.item())\n",
    "        accuracy_values_test.append(accuracy_test.item())\n",
    "        results.append(result)\n",
    "        \n",
    "\n",
    "    #Save model\n",
    "    torch.save(net, f'{pathName}/model.pth')\n",
    "\n",
    "    #Save plot loss\n",
    "    display.clear_output(wait=True)\n",
    "    plt.plot(loss_values_train, label='Training Loss')\n",
    "    plt.plot(loss_values_test, label = 'Test loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Loss per Epoch')\n",
    "    plt.legend()\n",
    "    plt.savefig(f'{pathName}/Loss.png')\n",
    "    plt.clf()\n",
    "\n",
    "    #Save plot accuracy\n",
    "    display.clear_output(wait=True)\n",
    "    plt.plot(accuracy_values_train, label='Accuracy Train')\n",
    "    plt.plot(accuracy_values_test, label='Accuracy Test')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Accuracy for Epoch')\n",
    "    plt.legend()\n",
    "    plt.savefig(f'{pathName}/Accuracy-test.png')\n",
    "    plt.clf()\n",
    "\n",
    "\n",
    "    with open(f\"{pathName}/results.txt\", 'w') as file:\n",
    "        for res in results:\n",
    "            file.write(res + \"\\n\")\n",
    "        bestPrint = f\"Best-Accuracy-Train: {best_accuracy_train:.4f}, Best-Accuracy-Test: {best_accuracy_test:.4f}\"\n",
    "        file.write(bestPrint + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
