{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import utils\n",
    "import torch\n",
    "import NetCup\n",
    "import statistics\n",
    "import numpy as np\n",
    "import LoadDataCup\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display as display\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "\n",
    "\n",
    "print(\"TRAINING CUP DATASET\")\n",
    "# PATH\n",
    "pathTrain = \"CUP/ML-CUP23-TRAIN.csv\"\n",
    "pathTestInput = \"CUP/ML-CUP23-TEST-INPUT.csv\"\n",
    "pathTestTarget = \"CUP/ML-CUP23-TEST-TARGET.csv\"\n",
    "seed = int(time.time()%150)\n",
    "# HYPERPARAMETER\n",
    "num_epochs = 2000\n",
    "#momentum = 0.9\n",
    "threshold = 0.01\n",
    "#penality = 0.0005\n",
    "\n",
    "#grid search\n",
    "layers_conf = [[10, 512, 512, 600, 3]]\n",
    "''' layers_conf = [[10, 100, 3],\n",
    "[10, 300, 3],\n",
    "[10, 100, 100, 3],\n",
    "[10, 300, 300, 3],\n",
    "[10, 10, 10, 20, 3],\n",
    "[10, 32, 64, 64, 3],\n",
    "[10, 40, 40, 80, 3],\n",
    "[10, 256, 256, 300, 3],\n",
    "[10, 256, 300, 256, 3],\n",
    "[10, 300, 256, 256, 3],\n",
    "[10, 512, 512, 600, 3],\n",
    "[10, 512, 1024, 2048, 3],\n",
    "[10, 64, 128, 200, 128, 3], \n",
    "[10, 80, 80, 80, 80, 80, 3],\n",
    "[10, 256, 512, 768, 1024, 3],\n",
    "[10, 256, 512, 768, 1024, 1280, 3]] '''\n",
    "activation_functions = ['tanh']\n",
    "optimizers = ['sgd']\n",
    "#penalities = [0.001, 0.0005, 0.0001, 0.0002]\n",
    "penalities = [0.0005]\n",
    "momentums = [0.8]\n",
    "#momentums = [0.9, 0.8, 0.6]\n",
    "#learning_rates = [0.001, 0.003, 0.0001, 0.0005] \n",
    "learning_rates = [0.003]\n",
    "#\n",
    "numberTest = len(layers_conf) * len(activation_functions) * len(optimizers) * len(penalities) * len(momentums) * len(learning_rates)\n",
    "bestResults = []\n",
    "\n",
    "# IMPORT DATA\n",
    "dataCup = LoadDataCup.DataCup(pathTrain)\n",
    "#Split Data\n",
    "dataCup.splitData()\n",
    "# DATA: TENSOR, GPU, DATALOADER\n",
    "dataCup.convertToTensor()\n",
    "# MOVE TO GPU\n",
    "#device = \"cuda:0\"\n",
    "#dataCup.moveToGpu(device=device)\n",
    "###\n",
    "data_loader_train, data_loader_test = dataCup.createDataLoader()\n",
    "\n",
    "\n",
    "for number, config in enumerate(product(layers_conf, activation_functions, optimizers, penalities, momentums, learning_rates)):\n",
    "    layers, activation, optimizerName, penality, momentum, lr = config\n",
    "    # PATH\n",
    "    testName = f\"{layers}-{optimizerName}-{activation}-{penality}-{momentum}-{lr}\"\n",
    "    pathName = f'modelsCup/Cup-{testName}'\n",
    "    bestResults.append(testName + \"\\n\")\n",
    "\n",
    "    # CREATE DIR\n",
    "    os.makedirs(pathName, exist_ok=True)\n",
    "    \n",
    "    history_train = []\n",
    "    history_test = []\n",
    "    \n",
    "    history_distance_train = []\n",
    "    history_distance_test = []\n",
    "    \n",
    "    # CREATE NET\n",
    "    structureNet = []\n",
    "    # If you need to change the neurons number go to netCup.py\n",
    "    print(\"Load regressor [net]\")\n",
    "    net = NetCup.NetCupRegressor(layers, structureNet, activation)\n",
    "    # MOVE NET TO GPU\n",
    "    #net = net.to(device)\n",
    "    # SET TYPE NET\n",
    "    net = net.float()\n",
    "    # OPTIMIZER AND CRITERION\n",
    "    # MSELoss for Regressor\n",
    "    # SGD for Regressor\n",
    "    print(\"Load MSELoss [criterion]\\nLoad SGD [optimizer]\")\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = None\n",
    "    if optimizerName == \"adam\":\n",
    "        optimizer = optim.Adam(net.parameters(), lr=lr, weight_decay=penality)\n",
    "    elif optimizerName == \"sgd\":\n",
    "        optimizer = optim.SGD(net.parameters(), lr=lr, momentum=momentum, weight_decay=penality)\n",
    "    else:\n",
    "        print(\"OPTIMIZER NON TROVATO!\")\n",
    "        exit(1)\n",
    "    \n",
    "    #Values used for graphs\n",
    "    loss_testues_train = []\n",
    "    accuracy_testues_train = []\n",
    "    loss_testues_test = []\n",
    "    accuracy_testues_test = []\n",
    "    euclidean_distances_train = []\n",
    "    euclidean_distances_test = []\n",
    "    # Distance list\n",
    "    euclidean_distance_train = []\n",
    "    euclidean_distance_test = []\n",
    "    # BEST\n",
    "    best_accuracy_train = 0.0\n",
    "    best_accuracy_test = 0.0\n",
    "    best_loss_train = 100.0\n",
    "    best_loss_test = 100.0\n",
    "    #\n",
    "    results = []\n",
    "    net.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        total = 0\n",
    "        correct = 0\n",
    "\n",
    "        for batch_input, batch_output in data_loader_train:\n",
    "            #Forward pass\n",
    "            outputs = net(batch_input)\n",
    "            #Training loss\n",
    "            loss = criterion(outputs, batch_output)\n",
    "            #Calculate total loss\n",
    "            total_loss += loss.item()\n",
    "            #Backward and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            #Take distance\n",
    "            distance = utils.euclidean_distance_loss(batch_output, outputs)\n",
    "            #Add distance to others\n",
    "            euclidean_distance_train.append(distance.item())\n",
    "        avg_loss_train = total_loss / len(data_loader_train)\n",
    "        loss_testues_train.append(avg_loss_train)\n",
    "\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        #CALCULATE ACCURACY test\n",
    "        net.eval()\n",
    "        total_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_input, batch_output in data_loader_test:\n",
    "                outputs = net(batch_input)\n",
    "                loss = criterion(outputs, batch_output)\n",
    "                total_loss += loss.item()\n",
    "                #Take distance\n",
    "                distance = utils.euclidean_distance_loss(batch_output, outputs)\n",
    "                #Add distance to others\n",
    "                euclidean_distance_test.append(distance.item())\n",
    "                \n",
    "            # Mean distance\n",
    "            mean_distance_train = statistics.mean(euclidean_distance_train)\n",
    "            mean_distance_test = statistics.mean(euclidean_distance_test)\n",
    "            # Mean loss\n",
    "            avg_loss_train = total_loss / len(data_loader_train)\n",
    "            avg_loss_test = total_loss / len(data_loader_test)\n",
    "            # Add to list\n",
    "            euclidean_distances_train.append(mean_distance_train)\n",
    "            euclidean_distances_test.append(mean_distance_test)\n",
    "            loss_testues_test.append(avg_loss_test)\n",
    "        net.train()\n",
    "        \n",
    "        result = f'Epoch[{epoch+1}/{num_epochs}] Learning-rate: {lr}, Loss-Train: {avg_loss_train:.4f}, Loss-test: {avg_loss_test:.4f} MEE-Train: {mean_distance_train:.4f}, MEE-test: {mean_distance_test:.4f}'\n",
    "        print(f\"Test[{number+1}/{numberTest}] --> \" + result)\n",
    "        \n",
    "        #Set best loss\n",
    "        best_loss_train = min(best_loss_train, avg_loss_train)\n",
    "        best_loss_test = min(best_loss_test, avg_loss_test)\n",
    "        #List append\n",
    "        results.append(result) \n",
    "\n",
    "    #END EPOCHS\n",
    "    \n",
    "    #History loss\n",
    "    history_train.append(loss_testues_train)\n",
    "    history_test.append(loss_testues_test)\n",
    "    #History distance\n",
    "    history_distance_train.append(euclidean_distances_train)\n",
    "    history_distance_test.append(euclidean_distances_test)\n",
    "    \n",
    "    #Save best results\n",
    "    bestPrint = f'     Best-loss-train: {best_loss_train:.4f}, Best-loss-test: {best_loss_test:.4f} \\n'\n",
    "    bestResults.append(bestPrint)\n",
    "    \n",
    "    with open(f\"{pathName}/results.txt\", 'w') as file:\n",
    "        for res in results:\n",
    "            file.write(res + \"\\n\")\n",
    "    #END KFOLD\n",
    "    \n",
    "    #Mean history\n",
    "    mean_train_loss = np.mean(history_train, axis=0)\n",
    "    mean_test_loss = np.mean(history_test, axis=0)\n",
    "    mean_train_mee = np.mean(history_distance_train, axis=0)\n",
    "    mean_test_mee = np.mean(history_distance_test, axis=0)\n",
    "    #Last loss\n",
    "    last_train_loss = [lst[-1] for lst in history_train]\n",
    "    last_test_loss = [lst[-1] for lst in history_test]\n",
    "    #Last MEE\n",
    "    last_mee_train = [lst[-1] for lst in history_distance_train]\n",
    "    last_mee_test = [lst[-1] for lst in history_distance_test]\n",
    "    #Mean last\n",
    "    mean_last_train_loss = np.mean(last_train_loss)\n",
    "    mean_last_test_loss = np.mean(last_test_loss)\n",
    "    mean_last_mee_train = np.mean(last_mee_train)\n",
    "    mean_last_mee_test = np.mean(last_mee_test)\n",
    "    #Adding best results\n",
    "    bestPrint = f\"     Mean-Last-Epoch-Train: {mean_last_train_loss:.4f}, Mean-Last-Epoch-test: {mean_last_test_loss:.4f}, MEE-Train: {mean_last_mee_train:.4f}, MEE-test: {mean_last_mee_test:.4f}\\n\"\n",
    "    bestResults.append(bestPrint)\n",
    "    \n",
    "    #Save plot loss\n",
    "    display.clear_output(wait=True)\n",
    "    plt.plot(mean_train_loss, label='Training Loss')\n",
    "    plt.plot(mean_test_loss, label = 'Test loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title(f'Mean-Loss per Epoch')\n",
    "    plt.ylim([0, 1.2])\n",
    "    plt.legend()\n",
    "    plt.savefig(f'{pathName}/Mean-Loss.png')\n",
    "    plt.clf()\n",
    "    \n",
    "    #Save plot loss\n",
    "    display.clear_output(wait=True)\n",
    "    plt.plot(mean_train_mee, label='MEE-Training')\n",
    "    plt.plot(mean_test_mee, label = 'MEE-Test')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('MEE')\n",
    "    plt.title(f'MEE per Epoch')\n",
    "    plt.ylim([0, 3.0])\n",
    "    plt.legend()\n",
    "    plt.savefig(f'{pathName}/MEE.png')\n",
    "    plt.clf()\n",
    "\n",
    "    #Save plot accuracy\n",
    "    ''' display.clear_output(wait=True)\n",
    "    plt.plot(accuracy_testues_train, label='Accuracy Train')\n",
    "    plt.plot(accuracy_testues_test, label='Accuracy Test')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Accuracy for Epoch kfold-{kfold}')\n",
    "    plt.legend()\n",
    "    plt.savefig(f'{pathName}/Accuracy-test.png')\n",
    "    plt.clf() '''\n",
    "    \n",
    "    #Save model\n",
    "    torch.save(net, f'{pathName}/model.pth')\n",
    "    \n",
    "    with open(f\"{pathName}/layer-structure.txt\", \"w\") as file:\n",
    "        for struct in structureNet:\n",
    "            file.write(struct + \"\\n\")\n",
    "    \n",
    "with open(\"Summary.txt\", \"w\") as file:\n",
    "    settings = f\"Grid Search Params: \\n Seed: {seed} \\n Layers-conf: {layers_conf} \\n Activation-function: {activation_functions} \\n Optimizers: {optimizers} \\n Lambdas: {penalities}\\n Momentums: {momentums}\\n Learning-rates: {learning_rates} \\n\\n\"\n",
    "    file.write(settings)\n",
    "    for best in bestResults:\n",
    "        file.write(best)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
