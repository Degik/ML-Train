{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING CUP DATASET\n",
      "Batch size for training:  64\n",
      "Batch size for testing:  64\n",
      "Load regressor [net]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "empty() received an invalid combination of arguments - got (tuple, dtype=NoneType, device=NoneType), but expected one of:\n * (tuple of ints size, *, tuple of names names, torch.memory_format memory_format, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (tuple of ints size, *, torch.memory_format memory_format, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 53\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# CREATE NET\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# If you need to change the neurons number go to netCup.py\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoad regressor [net]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 53\u001b[0m net \u001b[38;5;241m=\u001b[39m \u001b[43mNetCup\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mNetCupRegressor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayer1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer3\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m#net = NetCup.NetCupCNN()\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# MOVE NET TO GPU\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m#net = net.to(device)\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# SET TYPE NET\u001b[39;00m\n\u001b[1;32m     58\u001b[0m net \u001b[38;5;241m=\u001b[39m net\u001b[38;5;241m.\u001b[39mfloat()\n",
      "File \u001b[0;32m~/Desktop/ML-Train/NetCup.py:25\u001b[0m, in \u001b[0;36mNetCupRegressor.__init__\u001b[0;34m(self, hd_layer1, hd_layer2, hd_layer3, hd_layer4, activation)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer3 \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(hd_layer2, hd_layer3)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m#nn.init.kaiming_normal_(self.layer2.weight, mode='fan_in', nonlinearity='relu')\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m#nn.init.uniform_(self.layer2.weight, -0.7, 0.7)\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m#nn.init.constant_(self.layer2.bias, 0.01)\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m#Layer 3 Input: 100 Output: 25\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer4 \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLinear\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhd_layer3\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhd_layer4\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m#nn.init.kaiming_normal_(self.layer2.weight, mode='fan_in', nonlinearity='relu')\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m#nn.init.uniform_(self.layer3.weight, -0.7, 0.7)\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m#nn.init.constant_(self.layer3.bias, 0.01)\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer5 \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(hd_layer4, \u001b[38;5;241m3\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/linear.py:96\u001b[0m, in \u001b[0;36mLinear.__init__\u001b[0;34m(self, in_features, out_features, bias, device, dtype)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_features \u001b[38;5;241m=\u001b[39m in_features\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_features \u001b[38;5;241m=\u001b[39m out_features\n\u001b[0;32m---> 96\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m=\u001b[39m Parameter(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_features\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfactory_kwargs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m bias:\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;241m=\u001b[39m Parameter(torch\u001b[38;5;241m.\u001b[39mempty(out_features, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfactory_kwargs))\n",
      "\u001b[0;31mTypeError\u001b[0m: empty() received an invalid combination of arguments - got (tuple, dtype=NoneType, device=NoneType), but expected one of:\n * (tuple of ints size, *, tuple of names names, torch.memory_format memory_format, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (tuple of ints size, *, torch.memory_format memory_format, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import NetCup\n",
    "import LoadDataCup\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display as display\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "print(\"TRAINING CUP DATASET\")\n",
    "# HYPERPARAMETER\n",
    "#interval = 0.7\n",
    "learning_rate = 0.001\n",
    "num_epochs = 3000\n",
    "momentum = 0.9\n",
    "threshold = 0.01\n",
    "#penality = 0.0005\n",
    "\n",
    "#grid search\n",
    "layers_conf = [[32, 64, 128, 256], [16, 32, 64, 128], [50, 100, 150, 200]]\n",
    "activation_functions = ['relu', 'tanh']\n",
    "optimizers = ['adam', 'sgd']\n",
    "penalities = [0.0005, 0.005]\n",
    "#\n",
    "bestResults = []\n",
    "\n",
    "for layers, activation, optimizerName, penality in product(layers_conf, activation_functions, optimizers, penalities):\n",
    "    layer1 = layers[0]\n",
    "    layer2 = layers[1]\n",
    "    layer3 = layers[2]\n",
    "    layer4 = layers[3]\n",
    "    # PATH\n",
    "    pathTrain = \"CUP/ML-CUP23-TRAIN.csv\"\n",
    "    pathTestInput = \"CUP/ML-CUP23-TEST-INPUT.csv\"\n",
    "    pathTestTarget = \"CUP/ML-CUP23-TEST-TARGET.csv\"\n",
    "    testName = f\"[{layer1}-{layer2}-{layer3}-{layer4}]-{optimizerName}-{activation}-{penality}\"\n",
    "    pathName = f'modelsCup/Cup-{testName}'\n",
    "    # IMPORT DATA\n",
    "    dataCup = LoadDataCup.DataCup(pathTrain, pathTestInput, pathTestTarget)\n",
    "    # SPLIT SET\n",
    "    dataCup.splitData()\n",
    "    # DATA: TENSOR, GPU, DATALOADER\n",
    "    dataCup.convertToTensor()\n",
    "    # MOVE TO GPU\n",
    "    #device = \"mps\"\n",
    "    #dataCup.moveToGpu(device=device)\n",
    "    data_loader_train, data_loader_test = dataCup.createDataLoader()\n",
    "    # CREATE NET\n",
    "    # If you need to change the neurons number go to netCup.py\n",
    "    print(\"Load regressor [net]\")\n",
    "    net = NetCup.NetCupRegressor(layer1, layer2, layer3, layer4, activation)\n",
    "    #net = NetCup.NetCupCNN()\n",
    "    # MOVE NET TO GPU\n",
    "    #net = net.to(device)\n",
    "    # SET TYPE NET\n",
    "    net = net.float()\n",
    "    # OPTIMIZER AND CRITERION\n",
    "    # MSELoss for Regressor\n",
    "    # SGD for Regressor\n",
    "    print(\"Load MSELoss [criterion]\\nLoad SGD [optimizer]\")\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = None\n",
    "    if optimizerName == \"adam\":\n",
    "        optimizer = optim.Adam(net.parameters(), lr=learning_rate, weight_decay=penality)\n",
    "    elif optimizerName == \"sgd\":\n",
    "        optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=momentum, weight_decay=penality)\n",
    "    else:\n",
    "        print(\"OPTIMIZER NON TROVATO!\")\n",
    "        exit(1)\n",
    "\n",
    "    # CREATE DIR\n",
    "    os.makedirs(pathName, exist_ok=True)    \n",
    "\n",
    "    # MODEL SAVE\n",
    "    \"\"\"\n",
    "    with open(f'{pathName}/model_parameters.txt', 'w') as file:\n",
    "        file.write('Pesi layer1\\n')\n",
    "        file.write(str(net.layer1.weight.data) + '\\n')\n",
    "        file.write('Bias layer1\\n')\n",
    "        file.write(str(net.layer1.bias.data) + '\\n')\n",
    "        file.write('Pesi layer2\\n')\n",
    "        file.write(str(net.layer2.weight.data) + '\\n')\n",
    "        file.write('Bias layer2\\n')\n",
    "        file.write(str(net.layer2.bias.data) + '\\n')\n",
    "        file.write('Pesi layer3\\n')\n",
    "        file.write(str(net.layer3.weight.data) + '\\n')\n",
    "        file.write('Bias layer3\\n')\n",
    "        file.write(str(net.layer3.bias.data) + '\\n')\n",
    "        file.write('Pesi layer4\\n')\n",
    "        file.write(str(net.layer4.weight.data) + '\\n')\n",
    "        file.write('Bias layer4\\n')\n",
    "        file.write(str(net.layer4.bias.data) + '\\n')\n",
    "    \"\"\"\n",
    "    #Values used for graphs\n",
    "    loss_values_train = []\n",
    "    accuracy_values_train = []\n",
    "    loss_values_test = []\n",
    "    accuracy_values_test = []\n",
    "    # BEST\n",
    "    best_accuracy_train = 0.0\n",
    "    best_accuracy_test = 0.0\n",
    "    best_loss_train = 100.0\n",
    "    best_loss_test = 100.0\n",
    "    #\n",
    "    results = []\n",
    "    net.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        total = 0\n",
    "        correct = 0\n",
    "\n",
    "        for batch_input, batch_output in data_loader_train:\n",
    "            #Forward pass\n",
    "            outputs = net(batch_input)\n",
    "            #Training loss\n",
    "            loss = criterion(outputs, batch_output)\n",
    "            #Calculate total loss\n",
    "            total_loss += loss.item()\n",
    "            #Backward and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            #\n",
    "            difference = torch.abs(outputs - batch_output)\n",
    "            total += batch_input.size(0)\n",
    "            correct += torch.sum(difference < threshold)\n",
    "        accuracy_train = correct / total\n",
    "        avg_loss_train = total_loss / len(data_loader_train)\n",
    "        #Add to list\n",
    "        loss_values_train.append(avg_loss_train)\n",
    "\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        #CALCULATE ACCURACY VAL\n",
    "        net.eval()\n",
    "        total_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_input, batch_output in data_loader_test:\n",
    "                outputs = net(batch_input)\n",
    "                loss = criterion(outputs, batch_output)\n",
    "                total_loss += loss.item()\n",
    "                difference = torch.abs(outputs - batch_output)\n",
    "                total += batch_input.size(0)\n",
    "                correct += torch.sum(difference < threshold)\n",
    "            accuracy_test = correct / total\n",
    "            avg_loss_test = total_loss / len(data_loader_test)\n",
    "            loss_values_test.append(avg_loss_test)\n",
    "        net.train()\n",
    "        \n",
    "        result = f'Epoch[{epoch+1}/{num_epochs}] Learning-rate: {learning_rate}, Loss-Train: {avg_loss_train:.4f}, Loss-Test: {avg_loss_test:.4f}, Best-Accuracy-Train: {accuracy_train:.4f}, Best-Accuracy-Test: {accuracy_test:.4f}'\n",
    "        print(result)\n",
    "        \n",
    "        #Set best accuracy\n",
    "        best_accuracy_train = max(best_accuracy_train, accuracy_train)\n",
    "        best_accuracy_test = max(best_accuracy_test, accuracy_test)\n",
    "        #Set best loss\n",
    "        best_loss_train = min(best_loss_train, avg_loss_train)\n",
    "        best_loss_test = min(best_loss_test, avg_loss_test)\n",
    "        #List append\n",
    "        accuracy_values_train.append(accuracy_train.item())\n",
    "        accuracy_values_test.append(accuracy_test.item())\n",
    "        results.append(result)\n",
    "        \n",
    "\n",
    "    #Save model\n",
    "    torch.save(net, f'{pathName}/model.pth')\n",
    "    \n",
    "    #Save best results\n",
    "    bestPrint = f\"{testName}\\n   Best-loss-train: {best_loss_train:.4f}, Best-loss-test: {best_loss_test:.4f}, Best-Accuracy-Train: {best_accuracy_train:.4f}, Best-Accuracy-Test: {best_accuracy_test:.4f}\\n\"\n",
    "    bestResults.append(bestPrint)\n",
    "    \n",
    "    # Filter loss\n",
    "    #loss_values_train = utils.filterElement(loss_values_train)\n",
    "    #loss_values_test = utils.filterElement(loss_values_test)\n",
    "    \n",
    "    # Order in the same len\n",
    "    #min_len = min(len(loss_values_train), len(loss_values_test))\n",
    "    #loss_values_train = loss_values_train[:min_len]\n",
    "    #loss_values_test = loss_values_test[:min_len]\n",
    "\n",
    "    #Save plot loss\n",
    "    display.clear_output(wait=True)\n",
    "    plt.plot(loss_values_train, label='Training Loss')\n",
    "    plt.plot(loss_values_test, label = 'Test loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Loss per Epoch')\n",
    "    plt.ylim([0, 1.2])\n",
    "    plt.legend()\n",
    "    plt.savefig(f'{pathName}/Loss.png')\n",
    "    plt.clf()\n",
    "\n",
    "    #Save plot accuracy\n",
    "    display.clear_output(wait=True)\n",
    "    plt.plot(accuracy_values_train, label='Accuracy Train')\n",
    "    plt.plot(accuracy_values_test, label='Accuracy Test')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Accuracy for Epoch')\n",
    "    plt.legend()\n",
    "    plt.savefig(f'{pathName}/Accuracy-test.png')\n",
    "    plt.clf()\n",
    "\n",
    "\n",
    "    with open(f\"{pathName}/results.txt\", 'w') as file:\n",
    "        for res in results:\n",
    "            file.write(res + \"\\n\")\n",
    "    \n",
    "with open(\"Summary.txt\", \"w\") as file:\n",
    "    settings = f\"Grid Search Params: \\n {layers_conf} \\n {activation_functions} \\n {optimizers} \\n {penalities} \\n\\n\"\n",
    "    file.write(settings)\n",
    "    for best in bestResults:\n",
    "        file.write(best)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
