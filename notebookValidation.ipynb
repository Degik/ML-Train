{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import NetCup\n",
    "import numpy as np\n",
    "import LoadDataCupValidation\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import validationFunctions as vF\n",
    "import IPython.display as display\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "\n",
    "\n",
    "print(\"TRAINING CUP DATASET\")\n",
    "# PATH\n",
    "pathTrain = \"CUP/ML-CUP23-TRAIN.csv\"\n",
    "pathTestInput = \"CUP/ML-CUP23-TEST-INPUT.csv\"\n",
    "pathTestTarget = \"CUP/ML-CUP23-TEST-TARGET.csv\"\n",
    "seed = 15\n",
    "# HYPERPARAMETER\n",
    "num_epochs = 2000\n",
    "#momentum = 0.9\n",
    "threshold = 0.01\n",
    "#penality = 0.0005\n",
    "\n",
    "#grid search\n",
    "#layers_conf = [[10, 256, 256, 300, 3], [10, 40, 40, 80, 3], [10, 64, 128, 200, 128, 3]]\n",
    "layers_conf = [[10, 256, 256, 300, 3]]\n",
    "activation_functions = ['tanh']\n",
    "optimizers = ['sgd']\n",
    "#penalities = [0.001, 0.0005, 0.0001]\n",
    "penalities = [0.001]\n",
    "momentums = [0.9]\n",
    "#learning_rates = [0.001, 0.0008, 0.0005]\n",
    "learning_rates = [0.001]\n",
    "#\n",
    "k_folds = 4\n",
    "#\n",
    "numberTest = len(layers_conf) * len(activation_functions) * len(optimizers) * len(penalities) * len(momentums) * len(learning_rates)\n",
    "bestResults = []\n",
    "\n",
    "# IMPORT DATA\n",
    "dataCup = LoadDataCupValidation.DataCup(pathTrain, k_folds, seed)\n",
    "# DATA: TENSOR, GPU, DATALOADER\n",
    "dataCup.convertToTensor()\n",
    "# MOVE TO GPU\n",
    "device = \"cuda:0\"\n",
    "dataCup.moveToGpu(device=device)\n",
    "\n",
    "\n",
    "for number, config in enumerate(product(layers_conf, activation_functions, optimizers, penalities, momentums, learning_rates)):\n",
    "    layers, activation, optimizerName, penality, momentum, lr = config\n",
    "    # PATH\n",
    "    testName = f\"{layers}-{optimizerName}-{activation}-{penality}-{momentum}-{lr}\"\n",
    "    pathName = f'modelsCup/Cup-{testName}'\n",
    "    bestResults.append(testName + \"\\n\")\n",
    "\n",
    "    # CREATE DIR\n",
    "    os.makedirs(pathName, exist_ok=True)\n",
    "    \n",
    "    history_train = []\n",
    "    history_val = []\n",
    "    \n",
    "    for kfold in range(k_folds):\n",
    "        # CREATE NET\n",
    "        structureNet = []\n",
    "        # If you need to change the neurons number go to netCup.py\n",
    "        print(\"Load regressor [net]\")\n",
    "        net = NetCup.NetCupRegressor(layers, structureNet, activation)\n",
    "        # MOVE NET TO GPU\n",
    "        net = net.to(device)\n",
    "        # SET TYPE NET\n",
    "        net = net.float()\n",
    "        # OPTIMIZER AND CRITERION\n",
    "        # MSELoss for Regressor\n",
    "        # SGD for Regressor\n",
    "        print(\"Load MSELoss [criterion]\\nLoad SGD [optimizer]\")\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = None\n",
    "        if optimizerName == \"adam\":\n",
    "            optimizer = optim.Adam(net.parameters(), lr=lr, weight_decay=penality)\n",
    "        elif optimizerName == \"sgd\":\n",
    "            optimizer = optim.SGD(net.parameters(), lr=lr, momentum=momentum, weight_decay=penality)\n",
    "        else:\n",
    "            print(\"OPTIMIZER NON TROVATO!\")\n",
    "            exit(1)\n",
    "        \n",
    "        data_loader_train, data_loader_val = dataCup.createDataLoader(kfold)\n",
    "        #Values used for graphs\n",
    "        loss_values_train = []\n",
    "        accuracy_values_train = []\n",
    "        loss_values_val = []\n",
    "        accuracy_values_val = []\n",
    "        # BEST\n",
    "        best_accuracy_train = 0.0\n",
    "        best_accuracy_val = 0.0\n",
    "        best_loss_train = 100.0\n",
    "        best_loss_val = 100.0\n",
    "        #\n",
    "        results = []\n",
    "        net.train()\n",
    "        for epoch in range(num_epochs):\n",
    "            total_loss = 0\n",
    "            total = 0\n",
    "            correct = 0\n",
    "\n",
    "            for batch_input, batch_output in data_loader_train:\n",
    "                #Forward pass\n",
    "                outputs = net(batch_input)\n",
    "                #Training loss\n",
    "                loss = criterion(outputs, batch_output)\n",
    "                #Calculate total loss\n",
    "                total_loss += loss.item()\n",
    "                #Backward and optimization\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            avg_loss_train = total_loss / len(data_loader_train)\n",
    "            #Add to list\n",
    "            loss_values_train.append(avg_loss_train)\n",
    "\n",
    "            total = 0\n",
    "            correct = 0\n",
    "            #CALCULATE ACCURACY VAL\n",
    "            net.eval()\n",
    "            total_loss = 0\n",
    "            with torch.no_grad():\n",
    "                for batch_input, batch_output in data_loader_val:\n",
    "                    outputs = net(batch_input)\n",
    "                    loss = criterion(outputs, batch_output)\n",
    "                    total_loss += loss.item()\n",
    "                avg_loss_val = total_loss / len(data_loader_val)\n",
    "                loss_values_val.append(avg_loss_val)\n",
    "            net.train()\n",
    "            \n",
    "            result = f'KFold[{kfold+1}/{k_folds}] --> Epoch[{epoch+1}/{num_epochs}] Learning-rate: {lr}, Loss-Train: {avg_loss_train:.4f}, Loss-Val: {avg_loss_val:.4f}'\n",
    "            print(f\"GridSearch[{number+1}/{numberTest}] --> \" + result)\n",
    "            \n",
    "            #Set best loss\n",
    "            best_loss_train = min(best_loss_train, avg_loss_train)\n",
    "            best_loss_val = min(best_loss_val, avg_loss_val)\n",
    "            #List append\n",
    "            results.append(result) \n",
    "\n",
    "        #END EPOCHS\n",
    "        \n",
    "        history_train.append(loss_values_train)\n",
    "        history_val.append(loss_values_val)\n",
    "        \n",
    "        #Save best results\n",
    "        bestPrint = f'     KFold-{kfold} -> Best-loss-train: {best_loss_train:.4f}, Best-loss-val: {best_loss_val:.4f} \\n'\n",
    "        bestResults.append(bestPrint)\n",
    "        \n",
    "        with open(f\"{pathName}/results_kfold-{kfold}.txt\", 'w') as file:\n",
    "            for res in results:\n",
    "                file.write(res + \"\\n\")\n",
    "    #END KFOLD\n",
    "    \n",
    "    mean_train_loss = np.mean(history_train, axis=0)\n",
    "    mean_val_loss = np.mean(history_val, axis=0)\n",
    "    #Last\n",
    "    last_train_loss = [lst[-1] for lst in history_train]\n",
    "    last_val_loss = [lst[-1] for lst in history_val]\n",
    "    #Mean last\n",
    "    mean_last_train_loss = np.mean(last_train_loss)\n",
    "    mean_last_val_loss = np.mean(last_val_loss)\n",
    "    #Adding best results\n",
    "    bestPrint = f\"     Mean-Last-Epoch-Train: {mean_last_train_loss:.4f}, Mean-Last-Epoch-Val: {mean_last_val_loss:.4f}\\n\"\n",
    "    bestResults.append(bestPrint)\n",
    "    \n",
    "    #Save plot loss\n",
    "    display.clear_output(wait=True)\n",
    "    plt.plot(mean_train_loss, label='Training Loss')\n",
    "    plt.plot(mean_val_loss, label = 'Test loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title(f'Mean-Loss per Epoch')\n",
    "    plt.ylim([0, 1.2])\n",
    "    plt.legend()\n",
    "    plt.savefig(f'{pathName}/Mean-Loss.png')\n",
    "    plt.clf()\n",
    "    \n",
    "    #Save plot loss for training kfold\n",
    "    display.clear_output(wait=True)\n",
    "    for testNumber in range(k_folds):\n",
    "        plt.plot(history_train[testNumber], label=f'Train-Loss-{testNumber}')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title(f'Mean-Loss per Epoch')\n",
    "    plt.ylim([0, 1.2])\n",
    "    plt.legend()\n",
    "    plt.savefig(f'{pathName}/KFold-Loss-Train.png')\n",
    "    plt.clf()\n",
    "    \n",
    "    #Save plot loss for validation kfold\n",
    "    display.clear_output(wait=True)\n",
    "    display.clear_output(wait=True)\n",
    "    for testNumber in range(k_folds):\n",
    "        plt.plot(history_val[testNumber], label=f'Val-Loss-{testNumber}')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title(f'Mean-Loss per Epoch')\n",
    "    plt.ylim([0, 1.2])\n",
    "    plt.legend()\n",
    "    plt.savefig(f'{pathName}/KFold-Loss-Val.png')\n",
    "    plt.clf()\n",
    "\n",
    "    #Save plot accuracy\n",
    "    ''' display.clear_output(wait=True)\n",
    "    plt.plot(accuracy_values_train, label='Accuracy Train')\n",
    "    plt.plot(accuracy_values_val, label='Accuracy Test')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Accuracy for Epoch kfold-{kfold}')\n",
    "    plt.legend()\n",
    "    plt.savefig(f'{pathName}/Accuracy-test.png')\n",
    "    plt.clf() '''\n",
    "    \n",
    "    #Save model\n",
    "    torch.save(net, f'{pathName}/model.pth')\n",
    "    \n",
    "    with open(f\"{pathName}/layer-structure.txt\", \"w\") as file:\n",
    "        for struct in structureNet:\n",
    "            file.write(struct + \"\\n\")\n",
    "    \n",
    "with open(\"Summary.txt\", \"w\") as file:\n",
    "    settings = f\"Grid Search Params: \\n Layers-conf: {layers_conf} \\n Activation-function: {activation_functions} \\n Optimizers: {optimizers} \\n Lambdas: {penalities}\\n Momentums: {momentums}\\n Learning-rates: {learning_rates} \\n\\n\"\n",
    "    file.write(settings)\n",
    "    for best in bestResults:\n",
    "        file.write(best)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
