{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import utils\n",
    "import torch\n",
    "import NetCup\n",
    "import statistics\n",
    "import numpy as np\n",
    "import LoadDataCup\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display as display\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "\n",
    "\n",
    "print(\"TRAINING CUP DATASET\")\n",
    "# PATH\n",
    "pathTrain = \"CUP/ML-CUP23-TRAIN.csv\"\n",
    "pathTestInput = \"CUP/ML-CUP23-TEST-INPUT.csv\"\n",
    "pathTestTarget = \"CUP/ML-CUP23-TEST-TARGET.csv\"\n",
    "seed = int(time.time()%150)\n",
    "# HYPERPARAMETER\n",
    "num_epochs = 300\n",
    "#momentum = 0.9\n",
    "threshold = 0.01\n",
    "#penality = 0.0005\n",
    "\n",
    "#Netowrks\n",
    "network_configs = [\n",
    "    {\"layers\": [10, 256, 256, 300, 3], \"activation\": \"tanh\", \"optimizer\": \"sgd\", \"penality\": 0.0002, \"momentum\": 0.8, \"learning_rate\": 0.003},\n",
    "    {\"layers\": [10, 256, 256, 300, 3], \"activation\": \"tanh\", \"optimizer\": \"sgd\", \"penality\": 0.0005, \"momentum\": 0.8, \"learning_rate\": 0.003},\n",
    "    {\"layers\": [10, 300, 256, 256, 3], \"activation\": \"tanh\", \"optimizer\": \"sgd\", \"penality\": 0.0001, \"momentum\": 0.8, \"learning_rate\": 0.003},\n",
    "    {\"layers\": [10, 512, 512, 600, 3], \"activation\": \"tanh\", \"optimizer\": \"sgd\", \"penality\": 0.0001, \"momentum\": 0.8, \"learning_rate\": 0.003},\n",
    "    {\"layers\": [10, 256, 256, 300, 3], \"activation\": \"tanh\", \"optimizer\": \"sgd\", \"penality\": 0.0002, \"momentum\": 0.9, \"learning_rate\": 0.001},\n",
    "    {\"layers\": [10, 512, 512, 600, 3], \"activation\": \"tanh\", \"optimizer\": \"sgd\", \"penality\": 0.0005, \"momentum\": 0.8, \"learning_rate\": 0.003},\n",
    "    # Add here others config\n",
    "]\n",
    "#\n",
    "numberTest = len(network_configs)\n",
    "bestResults = []\n",
    "\n",
    "# IMPORT DATA\n",
    "dataCup = LoadDataCup.DataCup(pathTrain)\n",
    "#Split Data\n",
    "dataCup.splitData()\n",
    "# DATA: TENSOR, GPU, DATALOADER\n",
    "dataCup.convertToTensor()\n",
    "# MOVE TO GPU\n",
    "device = \"cuda:0\"\n",
    "#dataCup.moveToGpu(device=device)\n",
    "###\n",
    "data_loader_train, data_loader_test = dataCup.createDataLoader()\n",
    "\n",
    "#We will use this list for save all data for all training epoch and then calculate the mean\n",
    "history_train_loss = []\n",
    "history_train_mee = []\n",
    "history_test_loss = []\n",
    "history_test_mee = []\n",
    "\n",
    "\n",
    "for number, config in enumerate(network_configs):\n",
    "    #Net settings\n",
    "    layers = config['layers']\n",
    "    activation = config['activation']\n",
    "    optimizerName = config['optimizer']\n",
    "    penality = config['penality']\n",
    "    momentum = config['momentum']\n",
    "    lr = config['learning_rate']\n",
    "    # PATH\n",
    "    testName = f\"{layers}-{optimizerName}-{activation}-{penality}-{momentum}-{lr}\"\n",
    "    pathName = f'modelsCup/Cup-{testName}'\n",
    "    bestResults.append(testName + \"\\n\")\n",
    "\n",
    "    # CREATE DIR\n",
    "    os.makedirs(pathName, exist_ok=True)\n",
    "    \n",
    "    # CREATE NET\n",
    "    structureNet = []\n",
    "    # If you need to change the neurons number go to netCup.py\n",
    "    print(\"Load regressor [net]\")\n",
    "    net = NetCup.NetCupRegressor(layers, structureNet, activation)\n",
    "    # MOVE NET TO GPU\n",
    "    #net = net.to(device)\n",
    "    # SET TYPE NET\n",
    "    net = net.float()\n",
    "    # OPTIMIZER AND CRITERION\n",
    "    # MSELoss for Regressor\n",
    "    # SGD for Regressor\n",
    "    print(\"Load MSELoss [criterion]\\nLoad SGD [optimizer]\")\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = None\n",
    "    if optimizerName == \"adam\":\n",
    "        optimizer = optim.Adam(net.parameters(), lr=lr, weight_decay=penality)\n",
    "    elif optimizerName == \"sgd\":\n",
    "        optimizer = optim.SGD(net.parameters(), lr=lr, momentum=momentum, weight_decay=penality)\n",
    "    else:\n",
    "        print(\"OPTIMIZER NON TROVATO!\")\n",
    "        exit(1)\n",
    "    \n",
    "    #Values used for graphs\n",
    "    loss_values_train = []\n",
    "    accuracy_testues_train = []\n",
    "    loss_values_test = []\n",
    "    accuracy_testues_test = []\n",
    "    # Distance list\n",
    "    euclidean_distance_train = []\n",
    "    euclidean_distance_test = []\n",
    "    # Distances list\n",
    "    euclidean_distances_train = []\n",
    "    euclidean_distances_test = []\n",
    "    # BEST\n",
    "    best_accuracy_train = 0.0\n",
    "    best_accuracy_test = 0.0\n",
    "    best_loss_train = 100.0\n",
    "    best_loss_test = 100.0\n",
    "    #\n",
    "    results = []\n",
    "    net.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        \n",
    "        for batch_input, batch_output in data_loader_train:\n",
    "            #Forward pass\n",
    "            outputs = net(batch_input)\n",
    "            #Training loss\n",
    "            loss = criterion(outputs, batch_output)\n",
    "            #Calculate total loss\n",
    "            total_loss += loss.item()\n",
    "            #Backward and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            #Take distance\n",
    "            distance = utils.euclidean_distance_loss(batch_output, outputs)\n",
    "            #Add distance to others\n",
    "            euclidean_distance_train.append(distance.item())\n",
    "        avg_loss_train = total_loss / len(data_loader_train)\n",
    "        loss_values_train.append(avg_loss_train)\n",
    "\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        #CALCULATE ACCURACY test\n",
    "        net.eval()\n",
    "        total_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_input, batch_output in data_loader_test:\n",
    "                outputs = net(batch_input)\n",
    "                loss = criterion(outputs, batch_output)\n",
    "                total_loss += loss.item()\n",
    "                # Take distance\n",
    "                # Return the mean for the distance inside the batch\n",
    "                distance = utils.euclidean_distance_loss(batch_output, outputs)\n",
    "                # Add distance to others\n",
    "                # Take each distance for each bacth_size\n",
    "                euclidean_distance_test.append(distance.item())\n",
    "                \n",
    "            # Mean distance\n",
    "            # Calculate the mean between all batch inside the epoch\n",
    "            mean_distance_train = statistics.mean(euclidean_distance_train)\n",
    "            mean_distance_test = statistics.mean(euclidean_distance_test)\n",
    "            # Save the mean for the current epoch in the list\n",
    "            euclidean_distances_train.append(mean_distance_train)\n",
    "            euclidean_distances_test.append(mean_distance_test)\n",
    "            # Mean loss\n",
    "            avg_loss_train = total_loss / len(data_loader_train)\n",
    "            avg_loss_test = total_loss / len(data_loader_test)\n",
    "            loss_values_test.append(avg_loss_test)\n",
    "        net.train()\n",
    "        \n",
    "        result = f'Epoch[{epoch+1}/{num_epochs}] Learning-rate: {lr}, Loss-Train: {avg_loss_train:.4f}, Loss-test: {avg_loss_test:.4f} MEE-Train: {mean_distance_train:.4f}, MEE-test: {mean_distance_test:.4f}'\n",
    "        print(f\"Test[{number+1}/{numberTest}] --> \" + result)\n",
    "        \n",
    "        #Set best loss\n",
    "        best_loss_train = min(best_loss_train, avg_loss_train)\n",
    "        best_loss_test = min(best_loss_test, avg_loss_test)\n",
    "        #List append\n",
    "        results.append(result) \n",
    "\n",
    "    #END EPOCHS\n",
    "    \n",
    "    #History loss\n",
    "    history_train_loss.append(loss_values_train)\n",
    "    history_test_loss.append(loss_values_test)\n",
    "    #History distance\n",
    "    history_train_mee.append(euclidean_distances_train)\n",
    "    history_test_mee.append(euclidean_distances_test)\n",
    "    \n",
    "    #Save best results\n",
    "    bestPrint = f'     Best-loss-train: {best_loss_train:.4f}, Best-loss-test: {best_loss_test:.4f} \\n'\n",
    "    bestResults.append(bestPrint)\n",
    "    \n",
    "    with open(f\"{pathName}/results.txt\", 'w') as file:\n",
    "        for res in results:\n",
    "            file.write(res + \"\\n\")\n",
    "            \n",
    "    #Last loss\n",
    "    last_train_loss = loss_values_train[-1]\n",
    "    last_test_loss = loss_values_test[-1]\n",
    "    #Last MEE\n",
    "    last_mee_train = euclidean_distance_train[-1]\n",
    "    last_mee_test = euclidean_distance_test[-1]\n",
    "    #Adding best results\n",
    "    bestPrint = f\"     Mean-Last-Epoch-Train: {last_train_loss:.4f}, Mean-Last-Epoch-test: {last_test_loss:.4f}, MEE-Train: {last_mee_train:.4f}, MEE-test: {last_mee_test:.4f}\\n\"\n",
    "    bestResults.append(bestPrint)\n",
    "    \n",
    "    #Save plot loss\n",
    "    display.clear_output(wait=True)\n",
    "    plt.plot(loss_values_train, label='Training Loss')\n",
    "    plt.plot(loss_values_test, label = 'Test loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title(f'Mean-Loss per Epoch')\n",
    "    plt.ylim([0, 1.2])\n",
    "    plt.legend()\n",
    "    plt.savefig(f'{pathName}/Mean-Loss.png')\n",
    "    plt.clf()\n",
    "    \n",
    "    #Save plot loss\n",
    "    display.clear_output(wait=True)\n",
    "    plt.plot(euclidean_distances_train, label='MEE-Training')\n",
    "    plt.plot(euclidean_distances_test, label = 'MEE-Test')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('MEE')\n",
    "    plt.title(f'MEE per Epoch')\n",
    "    plt.ylim([0, 3.0])\n",
    "    plt.legend()\n",
    "    plt.savefig(f'{pathName}/MEE.png')\n",
    "    plt.clf()\n",
    "    \n",
    "    #Save model\n",
    "    torch.save(net, f'{pathName}/model.pth')\n",
    "    \n",
    "    with open(f\"{pathName}/layer-structure.txt\", \"w\") as file:\n",
    "        for struct in structureNet:\n",
    "            file.write(struct + \"\\n\")\n",
    "#END MODELS CONFIG\n",
    "\n",
    "#MEAN ALL DATA\n",
    "#Mean history\n",
    "mean_train_loss = np.mean(history_train_loss, axis=0)\n",
    "mean_test_loss = np.mean(history_test_loss, axis=0)\n",
    "mean_train_mee = np.mean(history_train_mee, axis=0)\n",
    "mean_test_mee = np.mean(history_test_mee, axis=0)\n",
    "#Last loss\n",
    "last_mean_train_loss = mean_train_loss[-1]\n",
    "last_mean_test_loss = mean_test_loss[-1]\n",
    "#Last MEE\n",
    "last_mean_train_mee = mean_train_mee[-1]\n",
    "last_mean_test_mee = mean_test_mee[-1]\n",
    "\n",
    "bestPrint = f\"MERGE-MODELS --> Mean-Last-Epoch-Train: {last_mean_train_loss:.4f}, Mean-Last-Epoch-test: {last_mean_test_loss:.4f}, MEE-Train: {last_mean_train_mee:.4f}, MEE-test: {last_mean_test_mee:.4f}\\n\"\n",
    "bestResults.append(bestPrint)\n",
    "\n",
    "#Save plot loss\n",
    "display.clear_output(wait=True)\n",
    "plt.plot(mean_train_loss, label='Training Loss')\n",
    "plt.plot(mean_test_loss, label = 'Test loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title(f'Mean-Loss per Epoch')\n",
    "plt.ylim([0, 1.2])\n",
    "plt.legend()\n",
    "plt.savefig(f'modelsCup/Mean-Loss-Merge.png')\n",
    "plt.clf()\n",
    "\n",
    "#Save plot loss\n",
    "display.clear_output(wait=True)\n",
    "plt.plot(mean_train_mee, label='MEE-Training')\n",
    "plt.plot(mean_test_mee, label = 'MEE-Test')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MEE')\n",
    "plt.title(f'MEE per Epoch')\n",
    "plt.ylim([0, 3.0])\n",
    "plt.legend()\n",
    "plt.savefig(f'modelsCup/MEE-Merge.png')\n",
    "plt.clf()\n",
    "\n",
    "\n",
    "with open(\"modelsCup/Summary.txt\", \"w\") as file:\n",
    "    file.write(\"Netoworks config: \" + \"\\n\")\n",
    "    for config in network_configs:\n",
    "        file.write(f\"  {str(config)} \\n\")\n",
    "    file.write(\"\\n\\n\\n\")\n",
    "    for best in bestResults:\n",
    "        file.write(best)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
