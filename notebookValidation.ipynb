{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import utils\n",
    "import torch\n",
    "import pandas as pd\n",
    "import validationFunctions as vF\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "pathTrain:str = \"CUP/ML-CUP23-TRAIN.csv\"\n",
    "seed:int = 21\n",
    "## TRAIN IMPORT DATASET\n",
    "dataset = utils.importDatasetCup(pathTrain)\n",
    "dataset_train, dataset_test = vF.train_test_split(dataset=dataset)\n",
    "# CREATE KFOLD\n",
    "dataset_train, dataset_val = vF.Kfold(dataset_train)\n",
    "#print(self.dataset_train)\n",
    "#print(self.dataset_val)\n",
    "# SPLIT FOLDS\n",
    "x_train, x_test, y_train, y_test = vF.split_folds(dataset_val, dataset_train) \n",
    "\n",
    "\n",
    "## CONVERT TO TENSOR TRAIN SET\n",
    "for i in range(4):\n",
    "    x_train[i] = torch.tensor(x_train[i].to_numpy())\n",
    "    y_train[i] = torch.tensor(y_train[i].to_numpy())\n",
    "    # SET TYPE DOUBLE\n",
    "    x_train[i] = x_train[i].double()\n",
    "    y_train[i] = y_train[i].double()\n",
    "    # CONVERT TO TENSOR\n",
    "    x_test[i] = torch.tensor(x_test[i].to_numpy())\n",
    "    y_test[i] = torch.tensor(y_test[i].to_numpy())\n",
    "    # SET TYPE DOUBLE\n",
    "    x_test[i] = x_test[i].double()\n",
    "    y_test[i] = y_test[i].double()\n",
    "\n",
    "for kfold in range(4):\n",
    "    # CREATE DATALOADER TRAIN\n",
    "    batchTrain =  600\n",
    "    print(\"Batch size for training: \", batchTrain)\n",
    "    dataset_train = TensorDataset(x_train[kfold], y_train[kfold])\n",
    "    data_loader_train = DataLoader(dataset_train, batch_size=batchTrain, shuffle=False)\n",
    "    # CREATE DATALOADER TEST\n",
    "    batchTest = 200\n",
    "    print(\"Batch size for testing: \", batchTest)\n",
    "    dataset_test = TensorDataset(x_test[kfold], y_test[kfold])\n",
    "    data_loader_test = DataLoader(dataset_test, batch_size=batchTest, shuffle=False)\n",
    "    \n",
    "    for batch_input, batch_output in data_loader_test:\n",
    "        print(batch_input)\n",
    "        print(batch_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING CUP DATASET\n",
      "Load regressor [net]\n",
      "Load MSELoss [criterion]\n",
      "Load SGD [optimizer]\n",
      "Batch size for training:  2\n",
      "Batch size for testing:  2\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "a Tensor with 2 elements cannot be converted to Scalar",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 135\u001b[0m\n\u001b[1;32m    133\u001b[0m     distance2 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean(distance)\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;66;03m#Add distance to others\u001b[39;00m\n\u001b[0;32m--> 135\u001b[0m     euclidean_distance_train\u001b[38;5;241m.\u001b[39mappend(\u001b[43mdistance\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    136\u001b[0m avg_loss_train \u001b[38;5;241m=\u001b[39m total_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(data_loader_train)\n\u001b[1;32m    137\u001b[0m loss_values_train\u001b[38;5;241m.\u001b[39mappend(avg_loss_train)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: a Tensor with 2 elements cannot be converted to Scalar"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import utils\n",
    "import torch\n",
    "import NetCup\n",
    "import statistics\n",
    "import numpy as np\n",
    "import LoadDataCupValidation\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import validationFunctions as vF\n",
    "import IPython.display as display\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "\n",
    "\n",
    "print(\"TRAINING CUP DATASET\")\n",
    "# PATH\n",
    "pathTrain = \"CUP/ML-CUP23-TRAIN.csv\"\n",
    "pathTestInput = \"CUP/ML-CUP23-TEST-INPUT.csv\"\n",
    "pathTestTarget = \"CUP/ML-CUP23-TEST-TARGET.csv\"\n",
    "seed = int(time.time()%150)\n",
    "# HYPERPARAMETER\n",
    "num_epochs = 10\n",
    "#momentum = 0.9\n",
    "threshold = 0.01\n",
    "#penality = 0.0005\n",
    "\n",
    "#grid search\n",
    "#layers_conf = [[10, 256, 256, 300, 3], [10, 40, 40, 80, 3], [10, 64, 128, 200, 128, 3]]\n",
    "layers_conf = [[10, 100, 100, 3]]\n",
    "activation_functions = ['tanh']\n",
    "optimizers = ['sgd']\n",
    "#penalities = [0.001, 0.0005, 0.0001]\n",
    "penalities = [0.0005]\n",
    "momentums = [0.9]\n",
    "#learning_rates = [0.001, 0.0008, 0.0005]\n",
    "learning_rates = [0.001]\n",
    "#\n",
    "k_folds = 4\n",
    "#\n",
    "numberTest = len(layers_conf) * len(activation_functions) * len(optimizers) * len(penalities) * len(momentums) * len(learning_rates)\n",
    "bestResults = []\n",
    "\n",
    "# IMPORT DATA\n",
    "dataCup = LoadDataCupValidation.DataCup(pathTrain, k_folds, seed)\n",
    "# DATA: TENSOR, GPU, DATALOADER\n",
    "dataCup.convertToTensor()\n",
    "# MOVE TO GPU\n",
    "#device = \"cuda:0\"\n",
    "#dataCup.moveToGpu(device=device)\n",
    "\n",
    "\n",
    "for number, config in enumerate(product(layers_conf, activation_functions, optimizers, penalities, momentums, learning_rates)):\n",
    "    layers, activation, optimizerName, penality, momentum, lr = config\n",
    "    # PATH\n",
    "    testName = f\"{layers}-{optimizerName}-{activation}-{penality}-{momentum}-{lr}\"\n",
    "    pathName = f'modelsCup/Cup1-{testName}'\n",
    "    bestResults.append(testName + \"\\n\")\n",
    "\n",
    "    # CREATE DIR\n",
    "    os.makedirs(pathName, exist_ok=True)\n",
    "    \n",
    "    history_train = []\n",
    "    history_val = []\n",
    "    \n",
    "    history_distance_train = []\n",
    "    history_distance_val = []\n",
    "    \n",
    "    for kfold in range(k_folds):\n",
    "        # CREATE NET\n",
    "        structureNet = []\n",
    "        # If you need to change the neurons number go to netCup.py\n",
    "        print(\"Load regressor [net]\")\n",
    "        net = NetCup.NetCupRegressor(layers, structureNet, activation)\n",
    "        # MOVE NET TO GPU\n",
    "        #net = net.to(device)\n",
    "        # SET TYPE NET\n",
    "        net = net.float()\n",
    "        # OPTIMIZER AND CRITERION\n",
    "        # MSELoss for Regressor\n",
    "        # SGD for Regressor\n",
    "        print(\"Load MSELoss [criterion]\\nLoad SGD [optimizer]\")\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = None\n",
    "        if optimizerName == \"adam\":\n",
    "            optimizer = optim.Adam(net.parameters(), lr=lr, weight_decay=penality)\n",
    "        elif optimizerName == \"sgd\":\n",
    "            optimizer = optim.SGD(net.parameters(), lr=lr, momentum=momentum, weight_decay=penality)\n",
    "        else:\n",
    "            print(\"OPTIMIZER NON TROVATO!\")\n",
    "            exit(1)\n",
    "        \n",
    "        data_loader_train, data_loader_val = dataCup.createDataLoader(kfold)\n",
    "        #Values used for graphs\n",
    "        loss_values_train = []\n",
    "        accuracy_values_train = []\n",
    "        loss_values_val = []\n",
    "        accuracy_values_val = []\n",
    "        euclidean_distances_train = []\n",
    "        euclidean_distances_val = []\n",
    "        # Distance list\n",
    "        euclidean_distance_train = []\n",
    "        euclidean_distance_val = []\n",
    "        # BEST\n",
    "        best_accuracy_train = 0.0\n",
    "        best_accuracy_val = 0.0\n",
    "        best_loss_train = 100.0\n",
    "        best_loss_val = 100.0\n",
    "        #\n",
    "        results = []\n",
    "        net.train()\n",
    "        for epoch in range(num_epochs):\n",
    "            total_loss = 0\n",
    "            total = 0\n",
    "            correct = 0\n",
    "\n",
    "            for batch_input, batch_output in data_loader_train:\n",
    "                #Forward pass\n",
    "                outputs = net(batch_input)\n",
    "                #Training loss\n",
    "                loss = criterion(outputs, batch_output)\n",
    "                #Calculate total loss\n",
    "                total_loss += loss.item()\n",
    "                #Backward and optimization\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                #Take distance\n",
    "                distance = utils.euclidean_distance_loss(batch_output, outputs)\n",
    "                #Add distance to others\n",
    "                euclidean_distance_train.append(distance.item())\n",
    "            avg_loss_train = total_loss / len(data_loader_train)\n",
    "            loss_values_train.append(avg_loss_train)\n",
    "\n",
    "            total = 0\n",
    "            correct = 0\n",
    "            #CALCULATE ACCURACY VAL\n",
    "            net.eval()\n",
    "            total_loss = 0\n",
    "            with torch.no_grad():\n",
    "                for batch_input, batch_output in data_loader_val:\n",
    "                    outputs = net(batch_input)\n",
    "                    loss = criterion(outputs, batch_output)\n",
    "                    total_loss += loss.item()\n",
    "                    #Take distance\n",
    "                    distance = utils.euclidean_distance_loss(batch_output, outputs)\n",
    "                    #Add distance to others\n",
    "                    euclidean_distance_val.append(distance.item())\n",
    "                    \n",
    "                # Mean distance\n",
    "                mean_distance_train = statistics.mean(euclidean_distance_train)\n",
    "                mean_distance_val = statistics.mean(euclidean_distance_val)\n",
    "                # Mean loss\n",
    "                avg_loss_train = total_loss / len(data_loader_train)\n",
    "                avg_loss_val = total_loss / len(data_loader_val)\n",
    "                # Add to list\n",
    "                euclidean_distances_train.append(mean_distance_train)\n",
    "                euclidean_distances_val.append(mean_distance_val)\n",
    "                loss_values_val.append(avg_loss_val)\n",
    "            net.train()\n",
    "            \n",
    "            result = f'KFold[{kfold+1}/{k_folds}] --> Epoch[{epoch+1}/{num_epochs}] Learning-rate: {lr}, Loss-Train: {avg_loss_train:.4f}, Loss-Val: {avg_loss_val:.4f} MEE-Train: {mean_distance_train:.4f}, MEE-Val: {mean_distance_val:.4f}'\n",
    "            print(f\"GridSearch[{number+1}/{numberTest}] --> \" + result)\n",
    "            \n",
    "            #Set best loss\n",
    "            best_loss_train = min(best_loss_train, avg_loss_train)\n",
    "            best_loss_val = min(best_loss_val, avg_loss_val)\n",
    "            #List append\n",
    "            results.append(result) \n",
    "\n",
    "        #END EPOCHS\n",
    "        \n",
    "        #History loss\n",
    "        history_train.append(loss_values_train)\n",
    "        history_val.append(loss_values_val)\n",
    "        #History distance\n",
    "        history_distance_train.append(euclidean_distances_train)\n",
    "        history_distance_val.append(euclidean_distances_val)\n",
    "        \n",
    "        #Save best results\n",
    "        bestPrint = f'     KFold-{kfold} -> Best-loss-train: {best_loss_train:.4f}, Best-loss-val: {best_loss_val:.4f} \\n'\n",
    "        bestResults.append(bestPrint)\n",
    "        \n",
    "        with open(f\"{pathName}/results_kfold-{kfold}.txt\", 'w') as file:\n",
    "            for res in results:\n",
    "                file.write(res + \"\\n\")\n",
    "    #END KFOLD\n",
    "    \n",
    "    #Mean history\n",
    "    mean_train_loss = np.mean(history_train, axis=0)\n",
    "    mean_val_loss = np.mean(history_val, axis=0)\n",
    "    mean_train_mee = np.mean(history_distance_train, axis=0)\n",
    "    mean_val_mee = np.mean(history_distance_val, axis=0)\n",
    "    #Last loss\n",
    "    last_train_loss = [lst[-1] for lst in history_train]\n",
    "    last_val_loss = [lst[-1] for lst in history_val]\n",
    "    #Last MEE\n",
    "    last_mee_train = [lst[-1] for lst in history_distance_train]\n",
    "    last_mee_val = [lst[-1] for lst in history_distance_val]\n",
    "    #Mean last\n",
    "    mean_last_train_loss = np.mean(last_train_loss)\n",
    "    mean_last_val_loss = np.mean(last_val_loss)\n",
    "    mean_last_mee_train = np.mean(last_mee_train)\n",
    "    mean_last_mee_val = np.mean(last_mee_val)\n",
    "    #Adding best results\n",
    "    bestPrint = f\"     Mean-Last-Epoch-Train: {mean_last_train_loss:.4f}, Mean-Last-Epoch-Val: {mean_last_val_loss:.4f}, MEE-Train: {mean_last_mee_train:.4f}, MEE-Val: {mean_last_mee_val:.4f}\\n\"\n",
    "    bestResults.append(bestPrint)\n",
    "    \n",
    "    #Save plot loss\n",
    "    display.clear_output(wait=True)\n",
    "    plt.plot(mean_train_loss, label='Training Loss')\n",
    "    plt.plot(mean_val_loss, label = 'Test loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title(f'Mean-Loss per Epoch')\n",
    "    plt.ylim([0, 1.2])\n",
    "    plt.legend()\n",
    "    plt.savefig(f'{pathName}/Mean-Loss.png')\n",
    "    plt.clf()\n",
    "    \n",
    "    #Save plot loss\n",
    "    display.clear_output(wait=True)\n",
    "    plt.plot(mean_train_mee, label='MEE-Training')\n",
    "    plt.plot(mean_val_mee, label = 'MEE-Test')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('MEE')\n",
    "    plt.title(f'MEE per Epoch')\n",
    "    plt.ylim([0, 3.0])\n",
    "    plt.legend()\n",
    "    plt.savefig(f'{pathName}/MEE.png')\n",
    "    plt.clf()\n",
    "    \n",
    "    #Save plot loss for training kfold\n",
    "    display.clear_output(wait=True)\n",
    "    for testNumber in range(k_folds):\n",
    "        plt.plot(history_train[testNumber], label=f'Train-Loss-{testNumber}')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title(f'Mean-Loss per Epoch')\n",
    "    plt.ylim([0, 1.2])\n",
    "    plt.legend()\n",
    "    plt.savefig(f'{pathName}/KFold-Loss-Train.png')\n",
    "    plt.clf()\n",
    "    \n",
    "    #Save plot loss for validation kfold\n",
    "    display.clear_output(wait=True)\n",
    "    display.clear_output(wait=True)\n",
    "    for testNumber in range(k_folds):\n",
    "        plt.plot(history_val[testNumber], label=f'Val-Loss-{testNumber}')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title(f'Mean-Loss per Epoch')\n",
    "    plt.ylim([0, 1.2])\n",
    "    plt.legend()\n",
    "    plt.savefig(f'{pathName}/KFold-Loss-Val.png')\n",
    "    plt.clf()\n",
    "\n",
    "    #Save plot accuracy\n",
    "    ''' display.clear_output(wait=True)\n",
    "    plt.plot(accuracy_values_train, label='Accuracy Train')\n",
    "    plt.plot(accuracy_values_val, label='Accuracy Test')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Accuracy for Epoch kfold-{kfold}')\n",
    "    plt.legend()\n",
    "    plt.savefig(f'{pathName}/Accuracy-test.png')\n",
    "    plt.clf() '''\n",
    "    \n",
    "    #Save model\n",
    "    torch.save(net, f'{pathName}/model.pth')\n",
    "    \n",
    "    with open(f\"{pathName}/layer-structure.txt\", \"w\") as file:\n",
    "        for struct in structureNet:\n",
    "            file.write(struct + \"\\n\")\n",
    "    \n",
    "with open(\"Summary.txt\", \"w\") as file:\n",
    "    settings = f\"Grid Search Params: \\n Seed: {seed} \\n Layers-conf: {layers_conf} \\n Activation-function: {activation_functions} \\n Optimizers: {optimizers} \\n Lambdas: {penalities}\\n Momentums: {momentums}\\n Learning-rates: {learning_rates} \\n\\n\"\n",
    "    file.write(settings)\n",
    "    for best in bestResults:\n",
    "        file.write(best)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
